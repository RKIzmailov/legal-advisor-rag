{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "config_path = '..\\\\config.yaml'\n",
    "\n",
    "\n",
    "with open(config_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully initialized gpt-4o-mini LLM client.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "\n",
    "load_dotenv(dotenv_path='..\\\\.env')\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"Missing OpenAI API key.\")\n",
    "if not ANTHROPIC_API_KEY:\n",
    "    raise ValueError(\"Missing Anthropic API key.\")\n",
    "\n",
    "openai_clent = OpenAI(api_key = OPENAI_API_KEY)\n",
    "anthropic_client = Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "\n",
    "llm_client_map = {\n",
    "    \"gpt-4o-mini\": openai_clent,\n",
    "    \"claude-3-sonnet-20240229\": anthropic_client\n",
    "}\n",
    "\n",
    "def get_llm_client(model_name):\n",
    "    \"\"\"\n",
    "    Get the appropriate LLM client based on the model name.\n",
    "    Raises an error if the model is not recognized.\n",
    "    \"\"\"\n",
    "    llm_class = llm_client_map.get(model_name)\n",
    "    if llm_class is None:\n",
    "        raise ValueError(f\"LLM client error: '{model_name}' is not recognized.\")\n",
    "    return llm_class\n",
    "\n",
    "model_name = config['RAG_model']['model_name']\n",
    "llm_client = get_llm_client(model_name)\n",
    "\n",
    "print(f\"Successfully initialized {model_name} LLM client.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Org =  config['Weaviate']['ORG']\n",
    "API_KEY = config['Weaviate']['API_KEY']\n",
    "weaviate_url =  config['Weaviate']['URL']\n",
    "\n",
    "chunk_size = config['Create_node']['chunk_size']\n",
    "chunk_overlap = config['Create_node']['chunk_overlap']\n",
    "\n",
    "documents_file_path = config['Documents_path']\n",
    "\n",
    "language = config['Language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rkizm\\.virtualenvs\\legal-advisor-rag-a8TsuGdb\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using embedding model: sentence-transformers/distiluse-base-multilingual-cased-v2\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "embed_model_name = config.get('Embedding_model', 'sentence-transformers/distiluse-base-multilingual-cased-v2')\n",
    "\n",
    "if \"sentence-transformers\" not in embed_model_name:\n",
    "    print(f\"Warning: The model {embed_model_name} is not a sentence-transformer model. Switching to a default.\")\n",
    "    embed_model_name = 'sentence-transformers/distiluse-base-multilingual-cased-v2'\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=embed_model_name)\n",
    "\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "print(f\"Using embedding model: {embed_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tx = 'how are you?'\n",
    "# embed_model.get_text_embedding(tx) == embed_model.get_query_embedding(tx)\n",
    "\n",
    "# emb1 = embed_model.get_text_embedding(tx)\n",
    "# emb2 = embed_model.get_query_embedding(tx)\n",
    "\n",
    "# embed_model.similarity(embedding1=emb1, embedding2=emb2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rkizm\\.virtualenvs\\legal-advisor-rag-a8TsuGdb\\Lib\\site-packages\\weaviate\\__init__.py:144: DeprecationWarning: Dep010: Importing AuthApiKey from weaviate is deprecated. Import AuthApiKey from its module: weaviate.auth\n",
      "  _Warnings.root_module_import(name, map_[name])\n",
      "C:\\Users\\rkizm\\AppData\\Local\\Temp\\ipykernel_17056\\3416258396.py:4: DeprecationWarning: \n",
      "Python client v3 `weaviate.Client(...)` connections and methods are deprecated and will\n",
      "            be removed by 2024-11-30.\n",
      "\n",
      "            Upgrade your code to use Python client v4 `weaviate.WeaviateClient` connections and methods.\n",
      "                - For Python Client v4 usage, see: https://weaviate.io/developers/weaviate/client-libraries/python\n",
      "                - For code migration, see: https://weaviate.io/developers/weaviate/client-libraries/python/v3_v4_migration\n",
      "\n",
      "            If you have to use v3 code, install the v3 client and pin the v3 dependency in your requirements file: `weaviate-client>=3.26.7;<4.0.0`\n",
      "  return weaviate.Client(\n",
      "c:\\Users\\rkizm\\.virtualenvs\\legal-advisor-rag-a8TsuGdb\\Lib\\site-packages\\weaviate\\warnings.py:162: DeprecationWarning: Dep016: Python client v3 `weaviate.Client(...)` connections and methods are deprecated and will\n",
      "            be removed by 2024-11-30.\n",
      "\n",
      "            Upgrade your code to use Python client v4 `weaviate.WeaviateClient` connections and methods.\n",
      "                - For Python Client v4 usage, see: https://weaviate.io/developers/weaviate/client-libraries/python\n",
      "                - For code migration, see: https://weaviate.io/developers/weaviate/client-libraries/python/v3_v4_migration\n",
      "\n",
      "            If you have to use v3 code, install the v3 client and pin the v3 dependency in your requirements file: `weaviate-client>=3.26.7;<4.0.0`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client is ready: True\n"
     ]
    }
   ],
   "source": [
    "# import weaviate\n",
    "\n",
    "# def get_weaviate_client(api_key, url):\n",
    "#     return weaviate.Client(\n",
    "#             url=weaviate_url,\n",
    "#             auth_client_secret=weaviate.AuthApiKey(api_key=api_key)\n",
    "#         )\n",
    "\n",
    "# # client\n",
    "# client = get_weaviate_client(API_KEY, weaviate_url)\n",
    "\n",
    "# print(f\"Client is ready: {client.is_ready()}\")\n",
    "\n",
    "# # DELETING all info from DB\n",
    "# client.schema.delete_class(Org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rkizm\\.virtualenvs\\legal-advisor-rag-a8TsuGdb\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weaviate client is ready: True\n",
      "Weaviate client connection closed.\n"
     ]
    }
   ],
   "source": [
    "# import weaviate\n",
    "# from llama_index.core import SimpleDirectoryReader, StorageContext, VectorStoreIndex, Document\n",
    "# from llama_index.vector_stores.weaviate import WeaviateVectorStore\n",
    "# from transformers import BertTokenizer\n",
    "# import re\n",
    "\n",
    "# def load_documents(file_path):\n",
    "#     return SimpleDirectoryReader(file_path).load_data()\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "\n",
    "# def count_tokens(text):\n",
    "#     \"\"\"Counting the number of tokens in a text\"\"\"\n",
    "#     tokens = tokenizer.tokenize(text)\n",
    "#     return len(tokens)\n",
    "\n",
    "\n",
    "# def split_large_text(text, max_tokens=4000):\n",
    "#     \"\"\"Split the text into fragments no larger than max_tokens\"\"\"\n",
    "#     words = text.split()\n",
    "#     split_texts = []\n",
    "#     current_chunk = []\n",
    "\n",
    "#     for word in words:\n",
    "#         current_chunk.append(word)\n",
    "#         if count_tokens(' '.join(current_chunk)) >= max_tokens:\n",
    "#             split_texts.append(' '.join(current_chunk))\n",
    "#             current_chunk = []\n",
    "\n",
    "#     if current_chunk:\n",
    "#         split_texts.append(' '.join(current_chunk))\n",
    "\n",
    "#     return split_texts\n",
    "\n",
    "\n",
    "# def split_text_by_paragraphs(text, max_tokens=4000):\n",
    "#     \"\"\"Splitting text into paragraphs of no more than max_tokens\"\"\"\n",
    "#     paragraphs = text.split('\\n\\n')\n",
    "#     new_nodes = []\n",
    "#     current_text = \"\"\n",
    "\n",
    "#     for para in paragraphs:\n",
    "#         if count_tokens(current_text + para) < max_tokens:\n",
    "#             current_text += para + \"\\n\\n\"\n",
    "#         else:\n",
    "#             if current_text:\n",
    "#                 new_nodes.append(current_text.strip())\n",
    "#             current_text = para + \"\\n\\n\"\n",
    "\n",
    "#     if current_text.strip():\n",
    "#         new_nodes.append(current_text.strip())\n",
    "\n",
    "#     final_nodes = []\n",
    "#     for node in new_nodes:\n",
    "#         if count_tokens(node) > max_tokens:\n",
    "#             split_nodes = split_large_text(node, max_tokens)\n",
    "#             final_nodes.extend(split_nodes)\n",
    "#         else:\n",
    "#             final_nodes.append(node)\n",
    "\n",
    "#     return final_nodes\n",
    "\n",
    "\n",
    "# def create_nodes(documents, max_tokens=4000):\n",
    "#     all_nodes = []\n",
    "#     for doc in documents:\n",
    "#         law_name = doc.metadata['file_name'].replace('.txt', '')\n",
    "#         law_text = doc.text\n",
    "#         nodes_list = re.split(r'\\n(?=Статья \\d+\\.)', law_text.strip())\n",
    "        \n",
    "#         for node_text in nodes_list:\n",
    "#             if count_tokens(node_text) <= max_tokens:\n",
    "#                 all_nodes.append(Document(text=node_text, metadata={'file_name': law_name}))\n",
    "#             else:\n",
    "#                 split_nodes = split_text_by_paragraphs(node_text, max_tokens=4000)\n",
    "#                 for split_node in split_nodes:\n",
    "#                     all_nodes.append(Document(text=split_node, metadata={'file_name': law_name}))\n",
    "\n",
    "#     return all_nodes\n",
    "\n",
    "\n",
    "# def connect_index(weaviate_client):\n",
    "#     vector_store = WeaviateVectorStore(\n",
    "#         weaviate_client=weaviate_client,\n",
    "#         index_name=Org\n",
    "#     )\n",
    "\n",
    "#     storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "#     index = VectorStoreIndex([], storage_context=storage_context)\n",
    "#     return index\n",
    "\n",
    "\n",
    "# def insert_nodes_index(index, nodes):\n",
    "#     index.insert_nodes(nodes)\n",
    "\n",
    "\n",
    "# client = weaviate.connect_to_wcs(\n",
    "#             cluster_url=weaviate_url,\n",
    "#             auth_credentials=weaviate.AuthApiKey(api_key=API_KEY)\n",
    "# )\n",
    "# print(\"Weaviate client is ready:\", client.is_ready())\n",
    "\n",
    "# documents = load_documents(documents_file_path)\n",
    "# nodes = create_nodes(documents)\n",
    "\n",
    "# index = connect_index(weaviate_client=client)\n",
    "# insert_nodes_index(index, nodes=nodes)\n",
    "\n",
    "# client.close()\n",
    "# print(\"Weaviate client connection closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rkizm\\.virtualenvs\\legal-advisor-rag-a8TsuGdb\\Lib\\site-packages\\weaviate\\__init__.py:144: DeprecationWarning: Dep010: Importing AuthApiKey from weaviate is deprecated. Import AuthApiKey from its module: weaviate.auth\n",
      "  _Warnings.root_module_import(name, map_[name])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client is ready: True\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "from llama_index.core import StorageContext, VectorStoreIndex\n",
    "from llama_index.vector_stores.weaviate import WeaviateVectorStore\n",
    "\n",
    "def connect_index(weaviate_client):\n",
    "    vector_store = WeaviateVectorStore(\n",
    "        weaviate_client=weaviate_client,\n",
    "        index_name=Org\n",
    "    )\n",
    "\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    index = VectorStoreIndex([], storage_context=storage_context)\n",
    "    return index\n",
    "\n",
    "weaviate_client = weaviate.connect_to_wcs(\n",
    "    cluster_url=weaviate_url,\n",
    "    auth_credentials=weaviate.AuthApiKey(api_key=API_KEY)\n",
    "    )\n",
    "print(\"Client is ready:\", weaviate_client.is_ready())\n",
    "\n",
    "index = connect_index(weaviate_client=weaviate_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index,\n",
    "    vector_store_query_mode=\"hybrid\",\n",
    "    similarity_top_k = config['Retriever']['similarity_top_k'],\n",
    "    alpha = 0.9, #config['Retriever']['alpha'],\n",
    "    similarity_threshold = config['Retriever']['similarity_threshold'],\n",
    "    )\n",
    "\n",
    "def search_nodes(query):\n",
    "    return retriever.retrieve(query)\n",
    "\n",
    "\n",
    "def build_prompt(query, search_results):\n",
    "\n",
    "    nodes_with_scores = [(node, node.score) for node in search_results]\n",
    "    sorted_nodes = sorted(nodes_with_scores, key=lambda x: x[1], reverse=True)\n",
    "    sorted_nodes_only = [node for node, score in sorted_nodes]\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for nod in sorted_nodes_only:\n",
    "        context = context + nod.metadata['file_name'].replace('txt', '') + '\\n' + nod.text + \"\\n\\n\"\n",
    "\n",
    "    prompt_template = config['RAG_model']['prompt_template']\n",
    "    prompt = prompt_template.format(language = language, question=query, context=context).strip()\n",
    "    return prompt\n",
    "\n",
    "def llm_response(prompt):\n",
    "    response = llm_client.chat.completions.create(\n",
    "        model=config['RAG_model']['model_name'],\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def rag(query):\n",
    "    search_results = search_nodes(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm_response(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Статья 380. Подделка, уничтожение идентификационного номера транспортного средства\n",
      " \n",
      "Подделка или уничтожение идентификационного номера, номера кузова, шасси, транспортного средства – \n",
      "наказываются штрафом от 500 до 1000 расчетных показателей либо лишением свободы на срок от двух до пяти лет.\n",
      " \n",
      "*** Статья 578. Ответственность за вред, причиненный транспортному средству\n",
      " \n",
      "В случае гибели или повреждения арендованного транспортного средства арендатор обязан возместить арендодателю причиненные убытки, если последний докажет, что гибель или повреждение транспортного средства произошли по обстоятельствам, за которые арендатор отвечает в соответствии с законом или договором аренды.\n",
      " \n"
     ]
    }
   ],
   "source": [
    "query = \"В каких случаях предусмотрена уголовная ответственность за подделку или уничтожение идентификационного номера транспортного средства, и какие виды наказания могут быть применены за совершение данного преступления?\"\n",
    "for res in search_nodes(query)[:2]:\n",
    "    print('***', res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criminal liability for the forgery or destruction of a vehicle's identification number is established in Article 380 of the Criminal Code of the Kyrgyz Republic. The punishment for this crime can include a fine ranging from 500 to 1000 calculated indicators or imprisonment for a term of two to five years.\n"
     ]
    }
   ],
   "source": [
    "answer = rag(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation-data-generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 3)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from random import randint\n",
    "\n",
    "# numder_of_articles = 50\n",
    "\n",
    "# f = True\n",
    "# while f:\n",
    "#     l = [randint(1, len(nodes)) for _ in range(numder_of_articles)]\n",
    "#     if len(set(l)) == numder_of_articles:\n",
    "#         f=False\n",
    "\n",
    "# sample = [nodes[i] for i in l]\n",
    "\n",
    "# data = []\n",
    "\n",
    "# for node in sample:\n",
    "#     data.append({\n",
    "#         'id': node.id_,\n",
    "#         'law_name': node.metadata['file_name'],\n",
    "#         'article': node.text\n",
    "#     })\n",
    "\n",
    "# df = pd.DataFrame(data)\n",
    "# df.to_csv('..\\\\data\\\\ground_truth.csv', index=False)\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('..\\\\data\\\\ground_truth.csv')\n",
    "# documents = df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question_generator_prompt_template = \"\"\"\n",
    "# You emulate a user of our legal advisor on legislation of Kyrgyz Republic application.\n",
    "# Formulate 3 questions this user might ask based on a provided article of the law.\n",
    "# Make the questions specific to the provided record. Don't reffer to the number of the article of the law.\n",
    "# The record should contain the answer to the questions, and the questions should be complete and not too short.\n",
    "# Use as fewer words as possible from the record. \n",
    "\n",
    "# The record:\n",
    "\n",
    "# Law name: {law_name}\n",
    "# Article of the law: {article}\n",
    "\n",
    "# Provide the output in parsable JSON without using code blocks:\n",
    "\n",
    "# {{\"questions\": [\"question1\", \"question2\", \"question3\"]}}\n",
    "# \"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6ca02fffda4983b54125e3cd289497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# def llm_response(prompt):\n",
    "#     response = llm_client.chat.completions.create(\n",
    "#         model=config['RAG_model']['model_name'],\n",
    "#         messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "#     )\n",
    "    \n",
    "#     return response.choices[0].message.content\n",
    "\n",
    "\n",
    "# def generate_questions(doc):\n",
    "#     question_generator_prompt = question_generator_prompt_template.format(**doc)\n",
    "#     json_response = llm_response(question_generator_prompt)\n",
    "#     return json_response\n",
    "\n",
    "\n",
    "# results = {}\n",
    "\n",
    "# for doc in tqdm(documents): \n",
    "#     doc_id = doc['id']\n",
    "#     if doc_id in results:\n",
    "#         continue\n",
    "\n",
    "#     questions_raw = generate_questions(doc)\n",
    "#     questions = json.loads(questions_raw)\n",
    "#     results[doc_id] = questions['questions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 2)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final_results = []\n",
    "\n",
    "# for doc_id, questions in results.items():\n",
    "#     for q in questions:\n",
    "#         final_results.append((doc_id, q))\n",
    "\n",
    "# df_questions = pd.DataFrame(final_results, columns=['id', 'question'])\n",
    "# df_questions.to_csv('../data/ground-truth-questions.csv', index=False)\n",
    "# df_questions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>law_name</th>\n",
       "      <th>article</th>\n",
       "      <th>new_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>74688912-792b-4de9-966a-ee2c76d91006</td>\n",
       "      <td>What can a lender demand if the borrower fails...</td>\n",
       "      <td>Гражданский Кодекс КР Часть II</td>\n",
       "      <td>Статья 730. Обеспечение исполнения обязательст...</td>\n",
       "      <td>3fdb1a03-dc6d-4c48-9d66-aa632b173948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>21ec195b-1ef9-4bfb-b4be-c2d8abf2e165</td>\n",
       "      <td>Is there a minimum period for filing an appell...</td>\n",
       "      <td>Гражданский процессуальный кодекс Кыргызской Р...</td>\n",
       "      <td>Статья 237. Обжалование заочного решения\\r\\n1....</td>\n",
       "      <td>d57aae71-032b-4a61-a513-e12f717ecbed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       id  \\\n",
       "9    74688912-792b-4de9-966a-ee2c76d91006   \n",
       "122  21ec195b-1ef9-4bfb-b4be-c2d8abf2e165   \n",
       "\n",
       "                                              question  \\\n",
       "9    What can a lender demand if the borrower fails...   \n",
       "122  Is there a minimum period for filing an appell...   \n",
       "\n",
       "                                              law_name  \\\n",
       "9                       Гражданский Кодекс КР Часть II   \n",
       "122  Гражданский процессуальный кодекс Кыргызской Р...   \n",
       "\n",
       "                                               article  \\\n",
       "9    Статья 730. Обеспечение исполнения обязательст...   \n",
       "122  Статья 237. Обжалование заочного решения\\r\\n1....   \n",
       "\n",
       "                                   new_id  \n",
       "9    3fdb1a03-dc6d-4c48-9d66-aa632b173948  \n",
       "122  d57aae71-032b-4a61-a513-e12f717ecbed  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Getting all nodes from weaviate\n",
    "\n",
    "# import weaviate\n",
    "\n",
    "# # Initialize the Weaviate client\n",
    "# client = weaviate.Client(\n",
    "#     url=weaviate_url,\n",
    "#     auth_client_secret=weaviate.AuthApiKey(api_key=API_KEY)\n",
    "# )\n",
    "\n",
    "# # Function to get all classes in the Weaviate instance\n",
    "# def get_all_classes(client):\n",
    "#     schema = client.schema.get()\n",
    "#     classes = [cls['class'] for cls in schema['classes']]\n",
    "#     return classes\n",
    "\n",
    "# # Function to get all properties of a specific class\n",
    "# def get_class_properties(client, class_name):\n",
    "#     schema = client.schema.get()\n",
    "#     for cls in schema['classes']:\n",
    "#         if cls['class'] == class_name:\n",
    "#             return [prop['name'] for prop in cls['properties']]\n",
    "#     return []\n",
    "\n",
    "# # Function to get all text nodes from a specific class with pagination\n",
    "# def get_all_text_nodes(client, class_name, properties):\n",
    "#     properties_str = ' '.join(properties)\n",
    "#     all_nodes = []\n",
    "#     limit = 100\n",
    "#     offset = 0\n",
    "\n",
    "#     while True:\n",
    "#         query = \"\"\"\n",
    "#         {\n",
    "#             Get {\n",
    "#                 %s(\n",
    "#                     limit: %d\n",
    "#                     offset: %d\n",
    "#                 ) {\n",
    "#                     %s\n",
    "#                     _additional {\n",
    "#                         id\n",
    "#                     }\n",
    "#                 }\n",
    "#             }\n",
    "#         }\n",
    "#         \"\"\" % (class_name, limit, offset, properties_str)\n",
    "\n",
    "#         response = client.query.raw(query)\n",
    "#         nodes = response['data']['Get'][class_name]\n",
    "\n",
    "#         if not nodes:\n",
    "#             break\n",
    "\n",
    "#         all_nodes.extend(nodes)\n",
    "#         offset += limit\n",
    "\n",
    "#     return all_nodes\n",
    "\n",
    "# # Get all classes\n",
    "# classes = get_all_classes(client)\n",
    "# print(\"Available classes:\", classes)\n",
    "\n",
    "# # Choose a class to query (for example, the first class)\n",
    "# if classes:\n",
    "#     class_name = Org #classes[0]\n",
    "#     properties = get_class_properties(client, class_name)\n",
    "#     print(f\"Properties of class '{class_name}':\", properties)\n",
    "\n",
    "#     # Get and print all text nodes from the chosen class\n",
    "#     if properties:\n",
    "#         text_nodes = get_all_text_nodes(client, class_name, properties)\n",
    "#         print(\"Number of text nodes retrieved:\", len(text_nodes))\n",
    "#         # Print first few nodes as an example\n",
    "#         print(\"First few text nodes:\", text_nodes[:5])\n",
    "#     else:\n",
    "#         print(f\"No properties found for class '{class_name}'.\")\n",
    "# else:\n",
    "#     print(\"No classes found in the Weaviate schema.\")\n",
    "\n",
    "\n",
    "# # merging questions with articles on 'id'\n",
    "# df_ar = pd.read_csv('..\\\\data\\\\ground_truth.csv')\n",
    "# df_questions = pd.read_csv('../data/ground-truth-questions.csv')\n",
    "# df = pd.merge(df_questions, df_ar, on='id', how='left')\n",
    "\n",
    "\n",
    "# # merging 'new_id' from text_nodes with questions\n",
    "# df_weaviate = pd.DataFrame(text_nodes)\n",
    "# df_weaviate._additional = df_weaviate._additional.apply(lambda i: i['id'])\n",
    "# df_weaviate=df_weaviate[['_additional', 'text']]\n",
    "# df_weaviate.columns = ['new_id', 'article']\n",
    "# df = pd.merge(df, df_weaviate, on='article', how='left')\n",
    "\n",
    "\n",
    "# df.to_csv('..\\\\data\\\\ground_truth_data.csv', index=False)\n",
    "# df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'question', 'law_name', 'article', 'new_id'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('..\\\\data\\\\ground_truth_data.csv')\n",
    "ground_truth = df.to_dict(orient='records')\n",
    "ground_truth[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7519fd6b15487c939423130077e5cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/405 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f016d8a358ad4471b9bfa8ad97ee6544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a130ca1426415699d6e69fadf9724f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c184ba71dd364d94b7e9a6b0f67536b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rkizm\\.virtualenvs\\legal-advisor-rag-a8TsuGdb\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c0860685b684e558a00a39b7cc96911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e09f22e076b84fc3afbd83a52d5c561c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.postprocessor.colbert_rerank import ColbertRerank\n",
    "\n",
    "colbert_reranker = ColbertRerank(\n",
    "    top_n=5,\n",
    "    model=\"colbert-ir/colbertv2.0\",\n",
    "    tokenizer=\"colbert-ir/colbertv2.0\",\n",
    "    keep_retrieval_score=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install git+https://github.com/alekssamos/yandexfreetranslate.git\n",
    "# %pip install yandexfreetranslate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yandexfreetranslate import YandexFreeTranslate\n",
    "yt = YandexFreeTranslate(api='ios') #Работает только так\n",
    "\n",
    "def ru(txt):\n",
    "  return yt.translate(\"en\", \"ru\", txt)\n",
    "\n",
    "def en(txt):\n",
    "  return yt.translate(\"ru\", \"en\", txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate(relevance_total):\n",
    "    cnt = 0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        if True in line:\n",
    "            cnt = cnt + 1\n",
    "\n",
    "    return cnt / len(relevance_total)\n",
    "\n",
    "\n",
    "def mrr(relevance_total):\n",
    "    total_score = 0.0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        for rank in range(len(line)):\n",
    "            if line[rank] == True:\n",
    "                total_score = total_score + 1 / (rank + 1)\n",
    "\n",
    "    return total_score / len(relevance_total)\n",
    "\n",
    "\n",
    "def evaluate(ground_truth, search_function):\n",
    "    relevance_total = []\n",
    "\n",
    "    for q in tqdm(ground_truth):\n",
    "        doc_id = q['new_id']\n",
    "        results = search_function(q)\n",
    "        relevance = [d.id_ == doc_id for d in results]\n",
    "        relevance_total.append(relevance)\n",
    "\n",
    "    return {\n",
    "        'hit_rate': hit_rate(relevance_total),\n",
    "        'mrr': mrr(relevance_total),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8261e13318754f718b98b2ef0ee958a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b682a8832c224086af066c6c3918f7be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b755104c6d84af28dc2e492e62ae1d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c0a0ebe68a64aeba7540e7b761bb114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cef8e330cb94686a706385236a5529e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "814b61d80e294de2a87a20e6e3575bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "852c05d8ce80439185bc4c83aa771f18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1129bd04f25c4ad5a6c86b2a168444c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bea9a7c19754f348ae788afe321c63b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d93518b3a046fd85698bcac4328144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0fe6060152440d29a9e2bc5de29fc86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02099b2141ae4e2bb128e6c32df8f5a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837161521bfa442d9237b6dc2124bd1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec52f17bfe0c430f8f93e6abb5b20cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c87d5c0b73453b821f897c1e302e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee64d07701b4d8e821ce304a0315acf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "983a342b97ef41349bdd50a39d01b071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab76cd9e9d84728872408461cb47f62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7171bfd522ab44c8b6d2c6e40d02d3c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1170e03ee2448abbf11c84116ee285c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3190c511c5a4f44a11e204dd3430378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e73fba0d440843e6b887f457e9333b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf099dd2f9ba491683df2ee0954c3e4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "542ffd333536426b83233a5049967104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07c17d550f64434ba131a56a28bfa86f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a40a4953a9c424f890f540db205242d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7bbfdff6bd4ecd898bff159c648baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "325701d9317e48c4a6f48ba7d3465493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c0905fc001949e0acddf4b50fdf3d42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5c6d87989de4ce1a73171cdcda67c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alphas = [0.1 * i for i in range(1, 11)]\n",
    "\n",
    "df_retrieval_eval = pd.DataFrame(columns=['alpha', 'hit_rate', 'mrr'])\n",
    "for alpha in alphas:\n",
    "    retriever = VectorIndexRetriever(\n",
    "        index,\n",
    "        vector_store_query_mode=\"hybrid\",\n",
    "        similarity_top_k = config['Retriever']['similarity_top_k'],\n",
    "        alpha = round(alpha, 1),\n",
    "        similarity_threshold = 5,\n",
    "        )\n",
    "    \n",
    "    def search_nodes(query):\n",
    "        return retriever.retrieve(query['question'])\n",
    "\n",
    "    eval_res = evaluate(ground_truth, search_nodes)\n",
    "\n",
    "    df_retrieval_eval.loc[len(df_retrieval_eval)] = {'alpha': round(alpha, 1),\n",
    "                                                    'hit_rate': eval_res['hit_rate'],\n",
    "                                                    'mrr':eval_res['mrr']}\n",
    "\n",
    "\n",
    "df_retrieval_eval_ru = pd.DataFrame(columns=['alpha', 'hit_rate_ru_query', 'mrr_ru_query'])\n",
    "for alpha in alphas:\n",
    "    retriever = VectorIndexRetriever(\n",
    "        index,\n",
    "        vector_store_query_mode=\"hybrid\",\n",
    "        similarity_top_k = config['Retriever']['similarity_top_k'],\n",
    "        alpha = round(alpha, 1),\n",
    "        similarity_threshold = 5,\n",
    "        )\n",
    "    \n",
    "    def search_nodes(query):\n",
    "        return retriever.retrieve(ru(query['question']))\n",
    "\n",
    "    eval_res = evaluate(ground_truth, search_nodes)\n",
    "\n",
    "    df_retrieval_eval_ru.loc[len(df_retrieval_eval_ru)] = {'alpha': round(alpha, 1),\n",
    "                                                           'hit_rate_ru_query': eval_res['hit_rate'],\n",
    "                                                           'mrr_ru_query':eval_res['mrr']}\n",
    "df_retrieval_eval = pd.merge(df_retrieval_eval, df_retrieval_eval_ru, on='alpha', how='left')\n",
    "\n",
    "\n",
    "df_retrieval_eval_rerank = pd.DataFrame(columns=['alpha', 'hit_rate_rerank', 'mrr_rerank'])\n",
    "for alpha in alphas:\n",
    "    retriever = VectorIndexRetriever(\n",
    "        index,\n",
    "        vector_store_query_mode=\"hybrid\",\n",
    "        similarity_top_k = config['Retriever']['similarity_top_k'],\n",
    "        alpha = round(alpha, 1),\n",
    "        similarity_threshold = 10,\n",
    "        node_postprocessors=[colbert_reranker]\n",
    "        )\n",
    "    \n",
    "    def search_nodes(query):\n",
    "        return retriever.retrieve(ru(query['question']))\n",
    "\n",
    "    eval_res = evaluate(ground_truth, search_nodes)\n",
    "\n",
    "    df_retrieval_eval_rerank.loc[len(df_retrieval_eval_rerank)] = {'alpha': round(alpha, 1),\n",
    "                                                                   'hit_rate_rerank': eval_res['hit_rate'],\n",
    "                                                                   'mrr_rerank':eval_res['mrr']}\n",
    "df_retrieval_eval = pd.merge(df_retrieval_eval, df_retrieval_eval_rerank, on='alpha', how='left') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "      <th>hit_rate_ru_query</th>\n",
       "      <th>mrr_ru_query</th>\n",
       "      <th>hit_rate_rerank</th>\n",
       "      <th>mrr_rerank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.626667</td>\n",
       "      <td>0.382622</td>\n",
       "      <td>0.686667</td>\n",
       "      <td>0.484405</td>\n",
       "      <td>0.686667</td>\n",
       "      <td>0.484405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.626667</td>\n",
       "      <td>0.382622</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.516323</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.516323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.626667</td>\n",
       "      <td>0.382622</td>\n",
       "      <td>0.753333</td>\n",
       "      <td>0.546304</td>\n",
       "      <td>0.753333</td>\n",
       "      <td>0.546304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.626667</td>\n",
       "      <td>0.382622</td>\n",
       "      <td>0.793333</td>\n",
       "      <td>0.563156</td>\n",
       "      <td>0.793333</td>\n",
       "      <td>0.563156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.389288</td>\n",
       "      <td>0.806667</td>\n",
       "      <td>0.564524</td>\n",
       "      <td>0.806667</td>\n",
       "      <td>0.564524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.389288</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.548487</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.548487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.389955</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.513161</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.513161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.390122</td>\n",
       "      <td>0.726667</td>\n",
       "      <td>0.487958</td>\n",
       "      <td>0.726667</td>\n",
       "      <td>0.487958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.390622</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.438357</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.438357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.390955</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.389579</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.389579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alpha  hit_rate       mrr  hit_rate_ru_query  mrr_ru_query  \\\n",
       "0    0.1  0.626667  0.382622           0.686667      0.484405   \n",
       "1    0.2  0.626667  0.382622           0.720000      0.516323   \n",
       "2    0.3  0.626667  0.382622           0.753333      0.546304   \n",
       "3    0.4  0.626667  0.382622           0.793333      0.563156   \n",
       "4    0.5  0.633333  0.389288           0.806667      0.564524   \n",
       "5    0.6  0.633333  0.389288           0.800000      0.548487   \n",
       "6    0.7  0.640000  0.389955           0.766667      0.513161   \n",
       "7    0.8  0.640000  0.390122           0.726667      0.487958   \n",
       "8    0.9  0.640000  0.390622           0.666667      0.438357   \n",
       "9    1.0  0.640000  0.390955           0.633333      0.389579   \n",
       "\n",
       "   hit_rate_rerank  mrr_rerank  \n",
       "0         0.686667    0.484405  \n",
       "1         0.720000    0.516323  \n",
       "2         0.753333    0.546304  \n",
       "3         0.793333    0.563156  \n",
       "4         0.806667    0.564524  \n",
       "5         0.800000    0.548487  \n",
       "6         0.766667    0.513161  \n",
       "7         0.726667    0.487958  \n",
       "8         0.666667    0.438357  \n",
       "9         0.633333    0.389579  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_retrieval_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translations of questions improves the rates of retriever. I will use alpha = 0.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legal-advisor-rag-x6CAXa7x",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
